{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Natural Language Toolkit: Distance Metrics\n",
    "#\n",
    "# Copyright (C) 2001-2019 NLTK Project\n",
    "# Author: Edward Loper <edloper@gmail.com>\n",
    "#         Steven Bird <stevenbird1@gmail.com>\n",
    "#         Tom Lippincott <tom@cs.columbia.edu>\n",
    "# URL: <http://nltk.org/>\n",
    "# For license information, see LICENSE.TXT\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "Distance Metrics.\n",
    "\n",
    "Compute the distance between two items (usually strings).\n",
    "As metrics, they must satisfy the following three requirements:\n",
    "\n",
    "1. d(a, a) = 0\n",
    "2. d(a, b) >= 0\n",
    "3. d(a, c) <= d(a, b) + d(b, c)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import warnings\n",
    "import operator\n",
    "\n",
    "def _edit_dist_init(len1, len2):\n",
    "    lev = []\n",
    "    for i in range(len1):\n",
    "        lev.append([0] * len2)  # initialize 2D array to zero\n",
    "    for i in range(len1):\n",
    "        lev[i][0] = i  # column 0: 0,1,2,3,4,...\n",
    "    for j in range(len2):\n",
    "        lev[0][j] = j  # row 0: 0,1,2,3,4,...\n",
    "    return lev\n",
    "\n",
    "\n",
    "def _edit_dist_step(lev, i, j, s1, s2, substitution_cost=1, transpositions=False):\n",
    "    c1 = s1[i - 1]\n",
    "    c2 = s2[j - 1]\n",
    "\n",
    "    # skipping a character in s1\n",
    "    a = lev[i - 1][j] + 1\n",
    "    # skipping a character in s2\n",
    "    b = lev[i][j - 1] + 1\n",
    "    # substitution\n",
    "    c = lev[i - 1][j - 1] + (substitution_cost if c1 != c2 else 0)\n",
    "\n",
    "    # transposition\n",
    "    d = c + 1  # never picked by default\n",
    "    if transpositions and i > 1 and j > 1:\n",
    "        if s1[i - 2] == c2 and s2[j - 2] == c1:\n",
    "            d = lev[i - 2][j - 2] + 1\n",
    "\n",
    "    # pick the cheapest\n",
    "    lev[i][j] = min(a, b, c, d)\n",
    "\n",
    "\n",
    "def edit_distance(s1, s2, substitution_cost=1, transpositions=False):\n",
    "    \"\"\"\n",
    "    Calculate the Levenshtein edit-distance between two strings.\n",
    "    The edit distance is the number of characters that need to be\n",
    "    substituted, inserted, or deleted, to transform s1 into s2.  For\n",
    "    example, transforming \"rain\" to \"shine\" requires three steps,\n",
    "    consisting of two substitutions and one insertion:\n",
    "    \"rain\" -> \"sain\" -> \"shin\" -> \"shine\".  These operations could have\n",
    "    been done in other orders, but at least three steps are needed.\n",
    "\n",
    "    Allows specifying the cost of substitution edits (e.g., \"a\" -> \"b\"),\n",
    "    because sometimes it makes sense to assign greater penalties to\n",
    "    substitutions.\n",
    "\n",
    "    This also optionally allows transposition edits (e.g., \"ab\" -> \"ba\"),\n",
    "    though this is disabled by default.\n",
    "\n",
    "    :param s1, s2: The strings to be analysed\n",
    "    :param transpositions: Whether to allow transposition edits\n",
    "    :type s1: str\n",
    "    :type s2: str\n",
    "    :type substitution_cost: int\n",
    "    :type transpositions: bool\n",
    "    :rtype int\n",
    "    \"\"\"\n",
    "    # set up a 2-D array\n",
    "    len1 = len(s1)\n",
    "    len2 = len(s2)\n",
    "    lev = _edit_dist_init(len1 + 1, len2 + 1)\n",
    "\n",
    "    # iterate over the array\n",
    "    for i in range(len1):\n",
    "        for j in range(len2):\n",
    "            _edit_dist_step(\n",
    "                lev,\n",
    "                i + 1,\n",
    "                j + 1,\n",
    "                s1,\n",
    "                s2,\n",
    "                substitution_cost=substitution_cost,\n",
    "                transpositions=transpositions,\n",
    "            )\n",
    "    return lev[len1][len2]\n",
    "\n",
    "\n",
    "\n",
    "def _edit_dist_backtrace(lev):\n",
    "    i, j = len(lev) - 1, len(lev[0]) - 1\n",
    "    alignment = [(i, j)]\n",
    "\n",
    "    while (i, j) != (0, 0):\n",
    "        directions = [\n",
    "            (i - 1, j),  # skip s1\n",
    "            (i, j - 1),  # skip s2\n",
    "            (i - 1, j - 1),  # substitution\n",
    "        ]\n",
    "\n",
    "        direction_costs = (\n",
    "            (lev[i][j] if (i >= 0 and j >= 0) else float('inf'), (i, j)) for i, j in directions\n",
    "        )\n",
    "        _, (i, j) = min(direction_costs, key=operator.itemgetter(0))\n",
    "\n",
    "        alignment.append((i, j))\n",
    "    return list(reversed(alignment))\n",
    "\n",
    "\n",
    "def edit_distance_align(s1, s2, substitution_cost=1):\n",
    "    \"\"\"\n",
    "    Calculate the minimum Levenshtein edit-distance based alignment\n",
    "    mapping between two strings. The alignment finds the mapping\n",
    "    from string s1 to s2 that minimizes the edit distance cost.\n",
    "    For example, mapping \"rain\" to \"shine\" would involve 2\n",
    "    substitutions, 2 matches and an insertion resulting in\n",
    "    the following mapping:\n",
    "    [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (4, 5)]\n",
    "    NB: (0, 0) is the start state without any letters associated\n",
    "    See more: https://web.stanford.edu/class/cs124/lec/med.pdf\n",
    "\n",
    "    In case of multiple valid minimum-distance alignments, the\n",
    "    backtrace has the following operation precedence:\n",
    "    1. Skip s1 character\n",
    "    2. Skip s2 character\n",
    "    3. Substitute s1 and s2 characters\n",
    "    The backtrace is carried out in reverse string order.\n",
    "\n",
    "    This function does not support transposition.\n",
    "\n",
    "    :param s1, s2: The strings to be aligned\n",
    "    :type s1: str\n",
    "    :type s2: str\n",
    "    :type substitution_cost: int\n",
    "    :rtype List[Tuple(int, int)]\n",
    "    \"\"\"\n",
    "    # set up a 2-D array\n",
    "    len1 = len(s1)\n",
    "    len2 = len(s2)\n",
    "    lev = _edit_dist_init(len1 + 1, len2 + 1)\n",
    "\n",
    "    # iterate over the array\n",
    "    for i in range(len1):\n",
    "        for j in range(len2):\n",
    "            _edit_dist_step(\n",
    "                lev,\n",
    "                i + 1,\n",
    "                j + 1,\n",
    "                s1,\n",
    "                s2,\n",
    "                substitution_cost=substitution_cost,\n",
    "                transpositions=False,\n",
    "            )\n",
    "\n",
    "    # backtrace to find alignment\n",
    "    alignment = _edit_dist_backtrace(lev)\n",
    "    return alignment\n",
    "\n",
    "\n",
    "\n",
    "def binary_distance(label1, label2):\n",
    "    \"\"\"Simple equality test.\n",
    "\n",
    "    0.0 if the labels are identical, 1.0 if they are different.\n",
    "\n",
    "    >>> from nltk.metrics import binary_distance\n",
    "    >>> binary_distance(1,1)\n",
    "    0.0\n",
    "\n",
    "    >>> binary_distance(1,3)\n",
    "    1.0\n",
    "    \"\"\"\n",
    "\n",
    "    return 0.0 if label1 == label2 else 1.0\n",
    "\n",
    "\n",
    "\n",
    "def jaccard_distance(label1, label2):\n",
    "    \"\"\"Distance metric comparing set-similarity.\n",
    "\n",
    "    \"\"\"\n",
    "    return (len(label1.union(label2)) - len(label1.intersection(label2))) / len(\n",
    "        label1.union(label2)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def masi_distance(label1, label2):\n",
    "    \"\"\"Distance metric that takes into account partial agreement when multiple\n",
    "    labels are assigned.\n",
    "\n",
    "    >>> from nltk.metrics import masi_distance\n",
    "    >>> masi_distance(set([1, 2]), set([1, 2, 3, 4]))\n",
    "    0.665\n",
    "\n",
    "    Passonneau 2006, Measuring Agreement on Set-Valued Items (MASI)\n",
    "    for Semantic and Pragmatic Annotation.\n",
    "    \"\"\"\n",
    "\n",
    "    len_intersection = len(label1.intersection(label2))\n",
    "    len_union = len(label1.union(label2))\n",
    "    len_label1 = len(label1)\n",
    "    len_label2 = len(label2)\n",
    "    if len_label1 == len_label2 and len_label1 == len_intersection:\n",
    "        m = 1\n",
    "    elif len_intersection == min(len_label1, len_label2):\n",
    "        m = 0.67\n",
    "    elif len_intersection > 0:\n",
    "        m = 0.33\n",
    "    else:\n",
    "        m = 0\n",
    "\n",
    "    return 1 - len_intersection / len_union * m\n",
    "\n",
    "\n",
    "\n",
    "def interval_distance(label1, label2):\n",
    "    \"\"\"Krippendorff's interval distance metric\n",
    "\n",
    "    >>> from nltk.metrics import interval_distance\n",
    "    >>> interval_distance(1,10)\n",
    "    81\n",
    "\n",
    "    Krippendorff 1980, Content Analysis: An Introduction to its Methodology\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        return pow(label1 - label2, 2)\n",
    "    #        return pow(list(label1)[0]-list(label2)[0],2)\n",
    "    except:\n",
    "        print(\"non-numeric labels not supported with interval distance\")\n",
    "\n",
    "\n",
    "\n",
    "def presence(label):\n",
    "    \"\"\"Higher-order function to test presence of a given label\n",
    "    \"\"\"\n",
    "\n",
    "    return lambda x, y: 1.0 * ((label in x) == (label in y))\n",
    "\n",
    "\n",
    "\n",
    "def fractional_presence(label):\n",
    "    return (\n",
    "        lambda x, y: abs(((1.0 / len(x)) - (1.0 / len(y))))\n",
    "        * (label in x and label in y)\n",
    "        or 0.0 * (label not in x and label not in y)\n",
    "        or abs((1.0 / len(x))) * (label in x and label not in y)\n",
    "        or ((1.0 / len(y))) * (label not in x and label in y)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def custom_distance(file):\n",
    "    data = {}\n",
    "    with open(file, 'r') as infile:\n",
    "        for l in infile:\n",
    "            labelA, labelB, dist = l.strip().split(\"\\t\")\n",
    "            labelA = frozenset([labelA])\n",
    "            labelB = frozenset([labelB])\n",
    "            data[frozenset([labelA, labelB])] = float(dist)\n",
    "    return lambda x, y: data[frozenset([x, y])]\n",
    "\n",
    "\n",
    "\n",
    "def jaro_similarity(s1, s2):\n",
    "    \"\"\"\n",
    "   Computes the Jaro similarity between 2 sequences from:\n",
    "\n",
    "        Matthew A. Jaro (1989). Advances in record linkage methodology\n",
    "        as applied to the 1985 census of Tampa Florida. Journal of the\n",
    "        American Statistical Association. 84 (406): 414-20.\n",
    "\n",
    "    The Jaro distance between is the min no. of single-character transpositions\n",
    "    required to change one word into another. The Jaro similarity formula from\n",
    "    https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance :\n",
    "\n",
    "        jaro_sim = 0 if m = 0 else 1/3 * (m/|s_1| + m/s_2 + (m-t)/m)\n",
    "\n",
    "    where:\n",
    "        - |s_i| is the length of string s_i\n",
    "        - m is the no. of matching characters\n",
    "        - t is the half no. of possible transpositions.\n",
    "\n",
    "    \"\"\"\n",
    "    # First, store the length of the strings\n",
    "    # because they will be re-used several times.\n",
    "    len_s1, len_s2 = len(s1), len(s2)\n",
    "\n",
    "    # The upper bound of the distance for being a matched character.\n",
    "    match_bound = max(len_s1, len_s2) // 2 - 1\n",
    "\n",
    "    # Initialize the counts for matches and transpositions.\n",
    "    matches = 0  # no.of matched characters in s1 and s2\n",
    "    transpositions = 0  # no. of transpositions between s1 and s2\n",
    "    flagged_1 = []  # positions in s1 which are matches to some character in s2\n",
    "    flagged_2 = []  # positions in s2 which are matches to some character in s1\n",
    "\n",
    "    # Iterate through sequences, check for matches and compute transpositions.\n",
    "    for i in range(len_s1):  # Iterate through each character.\n",
    "        upperbound = min(i + match_bound, len_s2 - 1)\n",
    "        lowerbound = max(0, i - match_bound)\n",
    "        for j in range(lowerbound, upperbound + 1):\n",
    "            if s1[i] == s2[j] and j not in flagged_2:\n",
    "                matches += 1\n",
    "                flagged_1.append(i)\n",
    "                flagged_2.append(j)\n",
    "                break\n",
    "    flagged_2.sort()\n",
    "    for i, j in zip(flagged_1, flagged_2):\n",
    "        if s1[i] != s2[j]:\n",
    "            transpositions += 1\n",
    "\n",
    "    if matches == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (\n",
    "            1\n",
    "            / 3\n",
    "            * (\n",
    "                matches / len_s1\n",
    "                + matches / len_s2\n",
    "                + (matches - transpositions // 2) / matches\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def jaro_winkler_similarity(s1, s2, p=0.1, max_l=4):\n",
    "    \"\"\"\n",
    "    The Jaro Winkler distance is an extension of the Jaro similarity in:\n",
    "\n",
    "        William E. Winkler. 1990. String Comparator Metrics and Enhanced\n",
    "        Decision Rules in the Fellegi-Sunter Model of Record Linkage.\n",
    "        Proceedings of the Section on Survey Research Methods.\n",
    "        American Statistical Association: 354-359.\n",
    "    such that:\n",
    "\n",
    "        jaro_winkler_sim = jaro_sim + ( l * p * (1 - jaro_sim) )\n",
    "\n",
    "    where,\n",
    "\n",
    "        - jaro_sim is the output from the Jaro Similarity,\n",
    "        see jaro_similarity()\n",
    "        - l is the length of common prefix at the start of the string\n",
    "            - this implementation provides an upperbound for the l value\n",
    "              to keep the prefixes.A common value of this upperbound is 4.\n",
    "        - p is the constant scaling factor to overweigh common prefixes.\n",
    "          The Jaro-Winkler similarity will fall within the [0, 1] bound,\n",
    "          given that max(p)<=0.25 , default is p=0.1 in Winkler (1990)\n",
    "\n",
    "\n",
    "    Test using outputs from https://www.census.gov/srd/papers/pdf/rr93-8.pdf\n",
    "    from \"Table 5 Comparison of String Comparators Rescaled between 0 and 1\"\n",
    "\n",
    "    >>> winkler_examples = [(\"billy\", \"billy\"), (\"billy\", \"bill\"), (\"billy\", \"blily\"),\n",
    "    ... (\"massie\", \"massey\"), (\"yvette\", \"yevett\"), (\"billy\", \"bolly\"), (\"dwayne\", \"duane\"),\n",
    "    ... (\"dixon\", \"dickson\"), (\"billy\", \"susan\")]\n",
    "\n",
    "    >>> winkler_scores = [1.000, 0.967, 0.947, 0.944, 0.911, 0.893, 0.858, 0.853, 0.000]\n",
    "    >>> jaro_scores =    [1.000, 0.933, 0.933, 0.889, 0.889, 0.867, 0.822, 0.790, 0.000]\n",
    "\n",
    "        # One way to match the values on the Winkler's paper is to provide a different\n",
    "    # p scaling factor for different pairs of strings, e.g.\n",
    "    >>> p_factors = [0.1, 0.125, 0.20, 0.125, 0.20, 0.20, 0.20, 0.15, 0.1]\n",
    "\n",
    "    >>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):\n",
    "    ...     assert round(jaro_similarity(s1, s2), 3) == jscore\n",
    "    ...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore\n",
    "\n",
    "\n",
    "    Test using outputs from https://www.census.gov/srd/papers/pdf/rr94-5.pdf from\n",
    "    \"Table 2.1. Comparison of String Comparators Using Last Names, First Names, and Street Names\"\n",
    "\n",
    "    >>> winkler_examples = [('SHACKLEFORD', 'SHACKELFORD'), ('DUNNINGHAM', 'CUNNIGHAM'),\n",
    "    ... ('NICHLESON', 'NICHULSON'), ('JONES', 'JOHNSON'), ('MASSEY', 'MASSIE'),\n",
    "    ... ('ABROMS', 'ABRAMS'), ('HARDIN', 'MARTINEZ'), ('ITMAN', 'SMITH'),\n",
    "    ... ('JERALDINE', 'GERALDINE'), ('MARHTA', 'MARTHA'), ('MICHELLE', 'MICHAEL'),\n",
    "    ... ('JULIES', 'JULIUS'), ('TANYA', 'TONYA'), ('DWAYNE', 'DUANE'), ('SEAN', 'SUSAN'),\n",
    "    ... ('JON', 'JOHN'), ('JON', 'JAN'), ('BROOKHAVEN', 'BRROKHAVEN'),\n",
    "    ... ('BROOK HALLOW', 'BROOK HLLW'), ('DECATUR', 'DECATIR'), ('FITZRUREITER', 'FITZENREITER'),\n",
    "    ... ('HIGBEE', 'HIGHEE'), ('HIGBEE', 'HIGVEE'), ('LACURA', 'LOCURA'), ('IOWA', 'IONA'), ('1ST', 'IST')]\n",
    "\n",
    "    >>> jaro_scores =   [0.970, 0.896, 0.926, 0.790, 0.889, 0.889, 0.722, 0.467, 0.926,\n",
    "    ... 0.944, 0.869, 0.889, 0.867, 0.822, 0.783, 0.917, 0.000, 0.933, 0.944, 0.905,\n",
    "    ... 0.856, 0.889, 0.889, 0.889, 0.833, 0.000]\n",
    "\n",
    "    >>> winkler_scores = [0.982, 0.896, 0.956, 0.832, 0.944, 0.922, 0.722, 0.467, 0.926,\n",
    "    ... 0.961, 0.921, 0.933, 0.880, 0.858, 0.805, 0.933, 0.000, 0.947, 0.967, 0.943,\n",
    "    ... 0.913, 0.922, 0.922, 0.900, 0.867, 0.000]\n",
    "\n",
    "        # One way to match the values on the Winkler's paper is to provide a different\n",
    "    # p scaling factor for different pairs of strings, e.g.\n",
    "    >>> p_factors = [0.1, 0.1, 0.1, 0.1, 0.125, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.20,\n",
    "    ... 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "\n",
    "\n",
    "    >>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):\n",
    "    ...     if (s1, s2) in [('JON', 'JAN'), ('1ST', 'IST')]:\n",
    "    ...         continue  # Skip bad examples from the paper.\n",
    "    ...     assert round(jaro_similarity(s1, s2), 3) == jscore\n",
    "    ...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore\n",
    "\n",
    "\n",
    "\n",
    "    This test-case proves that the output of Jaro-Winkler similarity depends on\n",
    "    the product  l * p and not on the product max_l * p. Here the product max_l * p > 1\n",
    "    however the product l * p <= 1\n",
    "\n",
    "    >>> round(jaro_winkler_similarity('TANYA', 'TONYA', p=0.1, max_l=100), 3)\n",
    "    0.88\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # To ensure that the output of the Jaro-Winkler's similarity\n",
    "    # falls between [0,1], the product of l * p needs to be\n",
    "    # also fall between [0,1].\n",
    "    if not 0 <= max_l * p <= 1:\n",
    "        warnings.warn(\n",
    "            str(\n",
    "                \"The product  `max_l * p` might not fall between [0,1].\"\n",
    "                \"Jaro-Winkler similarity might not be between 0 and 1.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Compute the Jaro similarity\n",
    "    jaro_sim = jaro_similarity(s1, s2)\n",
    "\n",
    "    # Initialize the upper bound for the no. of prefixes.\n",
    "    # if user did not pre-define the upperbound,\n",
    "    # use shorter length between s1 and s2\n",
    "\n",
    "    # Compute the prefix matches.\n",
    "    l = 0\n",
    "    # zip() will automatically loop until the end of shorter string.\n",
    "    for s1_i, s2_i in zip(s1, s2):\n",
    "        if s1_i == s2_i:\n",
    "            l += 1\n",
    "        else:\n",
    "            break\n",
    "        if l == max_l:\n",
    "            break\n",
    "    # Return the similarity value as described in docstring.\n",
    "    return jaro_sim + (l * p * (1 - jaro_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anagrams_3 = [\"THE\", \"WHY\", \"CAP\", \"YES\", \"CUP\", \"FUN\", \"EYE\", \"SUN\", \"HAT\", \"CAN\"]\n",
    "anagrams_4 = [\"WORD\", \"DEED\", \"BELL\", \"SORT\", \"PINT\", \"BELT\", \"CARD\", \"FIND\", \"YELL\", \"COLD\"]\n",
    "anagrams_5 = [\"VEGAN\", \"CLERK\", \"POKER\", \"OASIS\", \"GIANT\", \"OPIUM\", \"DONOR\", \"PHONE\", \"ELECT\", \"SCALE\"]\n",
    "anagrams_6 = [\"SHADOW\", \"DOLLAR\", \"RANDOM\", \"SALARY\", \"WEIGHT\", \"REFUND\", \"LETTER\",  \"CARBON\", \"WIZARD\", \"BALLET\"]\n",
    "anagrams_7 = [\"CLIMATE\", \"JAYWALK\", \"HAMMOCK\", \"DURABLE\", \"NAUGHTY\", \"GLACIER\", \"JUMBLED\", \"EXPLAIN\", \"GARBAGE\", \"JOURNEY\"]\n",
    "\n",
    "\n",
    "def shuffle_word(word): #shuffle function\n",
    "    word = list(word)\n",
    "    shuffle(word)\n",
    "    return ''.join(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"EAGVN\" 3\n",
      "\"LCKER\" 3\n",
      "\"KPORE\" 3\n",
      "\"ASOSI\" 3\n",
      "\"IAGTN\" 3\n",
      "\"IPUOM\" 3\n",
      "\"ORNOD\" 3\n",
      "\"NHOEP\" 3\n",
      "\"LEETC\" 3\n",
      "\"CAELS\" 3\n"
     ]
    }
   ],
   "source": [
    "dist = 3 # 3 for hard, 2 for easy\n",
    "#anagrams_6_shuffled = []\n",
    "\n",
    "for word in anagrams_5:    # go through all anagrams\n",
    "    d = 0 \n",
    "    t = 0\n",
    "    while (d != dist) and (t<=200): #if edit distance is not what we want then shuffle again, also do this for max 50 times        \n",
    "        if(dist == 2): #if edit distance is 2, then keep the first letter of the anagram same\n",
    "            w = shuffle_word(word[1:])\n",
    "            w = word[0]+w\n",
    "            d = edit_distance(word, w)\n",
    "        else:\n",
    "            w = shuffle_word(word)\n",
    "            while (w[0] == word[0]): #if first word same as before, keep shuffling\n",
    "                w = shuffle_word(word)\n",
    "            d = edit_distance(word, w)\n",
    "        t = t+1\n",
    "    print(\"\\\"\"+w+\"\\\"\", d)\n",
    "    #anagrams_7_shuffled.append(w)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "anagrams_4 = [\"WORD\", \"DEED\", \"BELL\", \"SORT\", \"PINT\", \"SNOW\", \"CARD\", \"FIND\", \"YELL\", \"COLD\", \"LOCK\", \"BEER\", \"COKE\", \"WHEN\", \"FAKE\", \"DATE\", \"WORN\",  \"BELT\", \"MAKE\", \"DOVE\", \"HOPE\", \"GONE\",  \"COME\", \"NICE\", \"GOOD\"]\n",
    "anagrams_5 = [\"VOLVO\", \"CLERK\", \"POKER\", \"OASIS\", \"VALUE\", \"OPIUM\", \"TAUNT\", \"APRON\", \"ELECT\", \"SCALE\",  \"COLON\", \"PATIO\", \"PUPIL\", \"GIANT\",\"AMAZE\",\"SWEET\", \"BACON\", \"VEGAN\", \"OCCUR\", \"AUDIT\", \"GUARD\",  \"BOUND\", \"GUESS\", \"PHONE\", \"DONOR\"]\n",
    "anagrams_6 = [\"UNIQUE\", \"SHADOW\", \"DOLLAR\", \"RANDOM\", \"KILLER\", \"BOTTLE\", \"WEIGHT\", \"REFUND\", \"LETTER\",  \"CARBON\", \"WIZARD\", \"BALLET\", \"BAMBOO\", \"DEFEAT\", \"DAZZLE\", \"DOCTOR\", \"FALCON\", \"FEMALE\", \"KITTEN\", \"NORWAY\", \"ORIGIN\", \"PALACE\", \"PUNISH\", \"ROOKIE\", \"SALARY\"]\n",
    "anagrams_7 = [\"TRIVIAL\",\"CLIMATE\", \"COMFORT\", \"JAYWALK\", \"HAMMOCK\", \"ORGANIC\", \"DURABLE\", \"NAUGHTY\", \"SQUEEZE\", \"JUMBLED\", \"CUPCAKE\", \"EXAMPLE\", \"CITIZEN\", \"CAMPING\", \"RUBBISH\",  \"EXPLAIN\", \"GLACIER\", \"JOURNEY\", \"POTLUCK\", \"GARBAGE\", \"PARADOX\", \"PLUMBER\", \"IMAGINE\", \"QUARREL\", \"SKETCHY\"]\n",
    "\n",
    "def shuffle_word(word): #shuffle function\n",
    "    word = list(word)\n",
    "    shuffle(word)\n",
    "    return ''.join(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLRIVIA 2\n",
      "CLIMETA 2\n",
      "COFMORT 2\n",
      "JAYLWAK 2\n",
      "HAMKOCM 2\n",
      "ORGIANC 2\n",
      "DURLABE 2\n",
      "NAGHUTY 2\n",
      "SEQUEEZ 2\n",
      "JMDBLEU 3\n",
      "CPCAEKU 3\n",
      "LXAEMPE 3\n",
      "IITZENC 3\n",
      "CNMAPIG 3\n",
      "RHUBBSI 3\n",
      "NEPLAIX 3\n",
      "RGLCIEA 3\n",
      "OURYJEN 4\n",
      "OTKLPUC 4\n",
      "RGABAEG 4\n",
      "AADOPRX 4\n",
      "MELUPBR 4\n",
      "IEAGNIM 4\n",
      "URRAELQ 4\n",
      "KYSTCHE 4\n"
     ]
    }
   ],
   "source": [
    "dist = np.concatenate([np.ones(9)*2, np.ones(8)*3, np.ones(8)*4], axis=0)\n",
    "c = 0\n",
    "anagrams_7_shuffled = []\n",
    "\n",
    "for word in anagrams_7:    # go through all anagrams\n",
    "    d = 0 \n",
    "    t = 0\n",
    "    while (d != dist[c]) and (t<=200): #if edit distance is not what we want then shuffle again, also do this for max 50 times        \n",
    "        if(dist[c] == 2): #if edit distance is 2, then keep the first letter of the anagram same\n",
    "            w = shuffle_word(word[1:])\n",
    "            w = word[0]+w\n",
    "            d = edit_distance(word, w)\n",
    "        else:\n",
    "            w = shuffle_word(word)\n",
    "            d = edit_distance(word, w)\n",
    "        t = t+1\n",
    "    c = c+1\n",
    "    print(w,d)\n",
    "    anagrams_7_shuffled.append(w)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(edit_distance('CLAIGER','GLACIER'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIVIAL & TLRIVIA :\n",
      "CLIMATE & CLIMETA :\n",
      "COMFORT & COFMORT :\n",
      "JAYWALK & JAYLWAK :\n",
      "HAMMOCK & HAMKOCM :\n",
      "ORGANIC & ORGIANC :\n",
      "DURABLE & DURLABE :\n",
      "NAUGHTY & NAGHUTY :\n",
      "SQUEEZE & SEQUEEZ :\n",
      "JUMBLED & JMDBLEU :\n",
      "CUPCAKE & CPCAEKU :\n",
      "EXAMPLE & LXAEMPE :\n",
      "CITIZEN & IITZENC :\n",
      "CAMPING & CNMAPIG :\n",
      "RUBBISH & RHUBBSI :\n",
      "EXPLAIN & NEPLAIX :\n",
      "GLACIER & RGLCIEA :\n",
      "JOURNEY & OURYJEN :\n",
      "POTLUCK & OTKLPUC :\n",
      "GARBAGE & RGABAEG :\n",
      "PARADOX & AADOPRX :\n",
      "PLUMBER & MELUPBR :\n",
      "IMAGINE & IEAGNIM :\n",
      "QUARREL & URRAELQ :\n",
      "SKETCHY & KYSTCHE :\n"
     ]
    }
   ],
   "source": [
    "anagrams_5 = [\"VOLVO\", \"CLERK\", \"POKER\", \"OASIS\", \"VALUE\", \"OPIUM\", \"TAUNT\", \"APRON\", \"ELECT\", \"SCALE\",  \"COLON\", \"PATIO\", \"PUPIL\", \"GIANT\",\"AMAZE\",\"SWEET\", \"BACON\", \"VEGAN\", \"OCCUR\", \"AUDIT\", \"GUARD\",  \"BOUND\", \"GUESS\", \"PHONE\", \"DONOR\"]\n",
    "\n",
    "anagrams_5_dist = [2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]\n",
    "\n",
    "anagrams_5_shuffled = [\"VLVOO\", \"CERKL\", \"PEKOR\", \"OSAIS\", \"VLUAE\", \"OMIUP\", \"TATUN\", \"APNRO\", \"EECLT\", \"ECSAL\", \"OLNOC\", \"PTAOI\", \"PLUIP\", \"IATNG\", \"AZAEM\", \"WTEES\", \"BCNOA\", \"EANVG\", \"RUCOC\", \"DTAIU\", \"URGDA\", \"UDONB\", \"USGSE\", \"OPEHN\", \"NDROO\"]\n",
    "\n",
    "anagrams_6 = [\"UNIQUE\", \"SHADOW\", \"DOLLAR\", \"RANDOM\", \"KILLER\", \"BOTTLE\", \"WEIGHT\", \"REFUND\", \"LETTER\",  \"CARBON\", \"WIZARD\", \"BALLET\", \"BAMBOO\", \"DEFEAT\", \"DAZZLE\", \"DOCTOR\", \"FALCON\", \"FEMALE\", \"KITTEN\", \"NORWAY\", \"ORIGIN\", \"PALACE\", \"PUNISH\", \"ROOKIE\", \"SALARY\"]\n",
    "\n",
    "anagrams_6_shuffled = [\"UUIQNE\", \"SHADWO\", \"DLLAOR\", \"RANDMO\", \"KELLIR\", \"BOLTTE\", \"WEHIGT\", \"RFUNDE\", \"LETTRE\", \"AROBCN\", \"DIZWRA\", \"BLELAT\", \"OABMBO\", \"EFETAD\", \"DZAEZL\", \"OCDOTR\", \"AOLCFN\", \"ALFEME\", \"ITTNKE\", \"NYWROA\", \"RNOGII\", \"PACELA\", \"SNUIPH\", \"REIOOK\", \"ALSYAR\"]\n",
    "\n",
    "anagrams_7 = [\"TRIVIAL\",\"CLIMATE\", \"COMFORT\", \"JAYWALK\", \"HAMMOCK\", \"ORGANIC\", \"DURABLE\", \"NAUGHTY\", \"SQUEEZE\", \"JUMBLED\", \"CUPCAKE\", \"EXAMPLE\", \"CITIZEN\", \"CAMPING\", \"RUBBISH\",  \"EXPLAIN\", \"GLACIER\", \"JOURNEY\", \"POTLUCK\", \"GARBAGE\", \"PARADOX\", \"PLUMBER\", \"IMAGINE\", \"QUARREL\", \"SKETCHY\"]\n",
    "\n",
    "anagrams_7_shuffled = [\"TLRIVIA\", \"CLIMETA\", \"COFMORT\", \"JAYLWAK\", \"HAMKOCM\", \"ORGIANC\", \"DURLABE\", \"NAGHUTY\", \"SEQUEEZ\", \"JMDBLEU\", \"CPCAEKU\", \"LXAEMPE\", \"IITZENC\", \"CNMAPIG\", \"RHUBBSI\", \"NEPLAIX\", \"RGLCIEA\", \"OURYJEN\", \"OTKLPUC\", \"RGABAEG\", \"AADOPRX\", \"MELUPBR\", \"IEAGNIM\", \"URRAELQ\", \"KYSTCHE\"]\n",
    "\n",
    "for i in range(25):    \n",
    "        print(anagrams_7[i], '&', anagrams_7_shuffled[i], ':')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DORNO\" 2\n",
      "\"SEALC\" 2\n",
      "\"PTLAN\" 2\n",
      "\"OSSAI\" 2\n",
      "\"CRAIH\" 2\n"
     ]
    }
   ],
   "source": [
    "dist = 2 # 3 for hard, 2 for easy\n",
    "anagrams = [\"LOCAL\", \"CLERK\", \"POKER\", \"PHONE\", \"MOUSE\"]\n",
    "anagrams2 = ['DONOR', 'SEACL', 'PLANT', 'OISSA', 'CHAIR']\n",
    "\n",
    "def shuffle_word(word): #shuffle function\n",
    "    word = list(word)\n",
    "    shuffle(word)\n",
    "    return ''.join(word)\n",
    "\n",
    "for word in anagrams2:    # go through all anagrams\n",
    "    d = 0 \n",
    "    t = 0\n",
    "    while (d != dist) and (t<=200): #if edit distance is not what we want then shuffle again, also do this for max 50 times        \n",
    "        #if edit distance is 2, then keep the first letter of the anagram same\n",
    "        w = shuffle_word(word[1:])\n",
    "        w = word[0]+w\n",
    "        d = edit_distance(word, w)        \n",
    "        t = t+1\n",
    "    print(\"\\\"\"+w+\"\\\"\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
